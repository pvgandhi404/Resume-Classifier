{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ZIbd9dE-RWgvGlDTLDKhe-fYsMHhzP2h",
      "authorship_tag": "ABX9TyMB/MdxWS4UwEtdlqbFAB2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvgandhi404/Resume-Classifier/blob/main/Adanomad_Text_CLassification_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Classifier"
      ],
      "metadata": {
        "id": "9e8rG8i33zks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resume classifier will catagorize a resume into 3 different job catagories:\n",
        "\n",
        "0: Software Engineer\n",
        "\n",
        "1: Data Scientist\n",
        "\n",
        "2: Project Manager\n",
        "\n",
        "\n",
        "To run the machine learning model, simply scroll down to the \"Model Testing\" section and hit run on both code blocks. Should an error be caused, please instead select \"run all\".  \n",
        "\n",
        " To insert custom inputs for the ML mode, simply change the variable \"text\" with the desired input. The code will automatically process the data and print the corresponding job title.\n",
        "\n",
        "\n",
        "Explanation:\n",
        "- The project was implemented using the Transformers library from HuggingFace and TensorFlow.\n",
        "\n",
        "- I was unable to find an appropriate dataset for project manager jobs. To ensure consistancy and prvent bias in the results, I decided to forgo using the datasets for software engineer and data scientist.\n",
        "\n",
        "- Instead, I simply asked chatGPT to give me a list of common job descriptions for each role. I then used that list to fine-tune the model.\n",
        "\n",
        "- Since the dataset is quite small, there is a likely chance of reduced accuracy due to both underfitting and overfitting data.\n",
        "\n",
        "Strategies:\n",
        "- To build the resume classifier, I used the \"finbert\" text classification model from Hugginface, mainly due to its popularity and ability to classify between 3 different labels. This allowed me to easily translate the model into classifying 3 different jobs.\n",
        "\n",
        "- I used the natural language took kit (ntlk) to remove all stopwords from all test and validation data. The same is done for all data before being processed by the model, thus increasing accuracy."
      ],
      "metadata": {
        "id": "TB70BdcG4FFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Installs\n",
        "\n"
      ],
      "metadata": {
        "id": "S1a666goDRCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers -q\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "Lu8C7zuwDV8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cc7e31-bb6f-4965-d583-848c22d0ab6a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TFTrainer, TFTrainingArguments, Trainer\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import TFTrainer, TFTrainingArguments"
      ],
      "metadata": {
        "id": "a4KsHXwgK1VW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Improvised Dataset: contains train_texts and train_labels (hidden for convenince)\n",
        "\n",
        "# 1rst 20 elements are software engineer descriptions\n",
        "# Middle 20 are data science descriptions\n",
        "# Last 20 are project manager descriptions\n",
        "train_texts = [\"Develop, test, and maintain software solutions according to specifications.\",\n",
        "\"Troubleshoot, debug, and resolve software defects and technical issues.\",\n",
        "\"Design and implement user-facing features using HTML, CSS, and JavaScript frameworks.\",\n",
        "\"Collaborate with UX/UI designers to create intuitive and responsive web interfaces.\",\n",
        "\"Optimize application performance and ensure cross-browser compatibility.\",\n",
        "\"Write efficient, reusable, and modular code for frontend components.\",\n",
        "\"Stay updated with emerging frontend technologies and industry trends.\",\n",
        "\"Build scalable, high-performance backend systems and APIs using programming languages like Python, Java, or Node.js.\",\n",
        "\"Design and implement database schemas and data models for efficient storage and retrieval.\",\n",
        "\"Ensure system security, data protection, and compliance with industry standards.\",\n",
        "\"Optimize backend processes for speed, scalability, and reliability.\",\n",
        "\"Collaborate with frontend developers to integrate frontend and backend components.\",\n",
        "\"Develop end-to-end web applications, handling both frontend and backend components.\",\n",
        "\"Architect and design software systems to meet functional and non-functional requirements.\",\n",
        "\"Implement features across the entire software stack, from database design to user interface development.\",\n",
        "\"Collaborate with product managers, designers, and other stakeholders to prioritize and deliver features.\",\n",
        "\"Conduct code reviews, testing, and debugging to ensure high-quality software delivery.\",\n",
        "\"Automate infrastructure provisioning, deployment, and management using tools like Docker, Kubernetes, and Terraform.\",\n",
        "\"Implement continuous integration and continuous deployment (CI/CD) pipelines to streamline software delivery.\",\n",
        "\"Monitor system performance, reliability, and availability using monitoring tools like Prometheus and Grafana.\",\n",
        "\n",
        "\"Analyze large datasets to extract actionable insights and drive business decisions.\",\n",
        "\"Develop machine learning models and algorithms for predictive analytics and forecasting.\",\n",
        "\"Collaborate with cross-functional teams to identify business opportunities and solve complex problems.\",\n",
        "\"Use statistical techniques and data visualization tools to communicate findings and recommendations.\",\n",
        "\"Continuously explore and implement new data science methodologies and technologies.\",\n",
        "\"Perform exploratory data analysis to uncover trends, patterns, and insights in data.\",\n",
        "\"Create dashboards, reports, and visualizations to present findings to stakeholders.\",\n",
        "\"Develop and maintain data pipelines for data extraction, transformation, and loading (ETL).\",\n",
        "\"Collaborate with business users to understand requirements and provide data-driven solutions.\",\n",
        "\"Conduct statistical analysis and hypothesis testing to support decision-making processes.\",\n",
        "\"Design, build, and maintain data infrastructure and pipelines for processing and storing large datasets.\",\n",
        "\"Develop and optimize data workflows for data ingestion, transformation, and storage.\",\n",
        "\"Implement data quality checks and monitoring to ensure data integrity and reliability.\",\n",
        "\"Collaborate with data scientists and analysts to support their data needs and requirements.\",\n",
        "\"Stay updated with emerging technologies and best practices in data engineering and big data.\",\n",
        "\"Gather and analyze business data to identify trends, opportunities, and challenges.\",\n",
        "\"Design and develop reports, dashboards, and interactive visualizations using BI tools.\",\n",
        "\"Translate business requirements into technical specifications for data queries and reports.\",\n",
        "\"Collaborate with stakeholders to define KPIs and metrics for measuring business performance.\",\n",
        "\"Provide insights and recommendations to support strategic decision-making.\",\n",
        "\n",
        "\"Plan, execute, and oversee project activities to ensure successful completion within scope, budget, and schedule.\",\n",
        "\"Define project objectives, deliverables, and milestones in collaboration with stakeholders.\",\n",
        "\"Allocate resources, manage budgets, and track project progress using project management tools.\",\n",
        "\"Identify risks, issues, and dependencies and implement mitigation strategies to minimize project disruptions.\",\n",
        "\"Facilitate communication and collaboration among project team members and stakeholders.\",\n",
        "\"Lead IT projects from initiation to closure, including software development, infrastructure upgrades, and system implementations.\",\n",
        "\"Coordinate with technical teams, business stakeholders, and vendors to deliver IT solutions that meet business requirements.\",\n",
        "\"Manage project scope, budget, and timelines while ensuring adherence to quality standards and regulatory requirements.\",\n",
        "\"Conduct project risk assessments and develop mitigation plans to address potential issues.\",\n",
        "\"Provide regular project status updates and reports to senior management and stakeholders.\",\n",
        "\"Lead infrastructure projects, such as network upgrades, data center migrations, and cloud migrations, from planning to implementation.\",\n",
        "\"Develop project plans, schedules, and budgets in alignment with organizational goals and technical requirements.\",\n",
        "\"Collaborate with cross-functional teams, including IT, operations, and facilities, to coordinate project activities and deliverables.\",\n",
        "\"Manage vendor relationships, contracts, and service level agreements (SLAs) to ensure project success and stakeholder satisfaction.\",\n",
        "\"Monitor project performance metrics and KPIs to track progress and identify areas for improvement.\",\n",
        "\"Facilitate agile ceremonies, including sprint planning, daily stand-ups, sprint reviews, and retrospectives, to support agile development methodologies.\",\n",
        "\"Manage agile project backlogs, prioritize user stories, and track sprint progress using agile project management tools.\",\n",
        "\"Coach and mentor agile teams on agile principles, practices, and tools to improve team performance and delivery outcomes.\",\n",
        "\"Collaborate with product owners, scrum masters, and stakeholders to define project goals, scope, and acceptance criteria.\",\n",
        "\"Foster a culture of continuous improvement and innovation within agile teams and across the organization.\"\n",
        "               ]\n",
        "\n",
        "# 0 = Software engineer\n",
        "# 1 = data scientist\n",
        "# 2 = project manager\n",
        "train_labels = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "                1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "                2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
        "\n",
        "print(len(train_texts))\n",
        "print(len(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9cOku4CBsb2Y",
        "outputId": "7bff5452-9f68-4dde-f5ec-c36baa4488e8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "qzmVsX5F1Keu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove all stop words from a string\n",
        "def remove_stopwords(text):\n",
        "  word_list = text.split()\n",
        "\n",
        "  filtered_list = [word for word in word_list if word not in stop_words]\n",
        "  return ' '.join(filtered_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3zNOxcGEV3D",
        "outputId": "43423830-c84c-4543-b26d-cd42f953df57"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all stopwords from training data\n",
        "for i in range(0, len(train_texts)):\n",
        "  train_texts[i] = remove_stopwords(train_texts[i])"
      ],
      "metadata": {
        "id": "jcLUNhdz1t96"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model and Save"
      ],
      "metadata": {
        "id": "Q-Od7KFz1QhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize training data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Prepare TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).shuffle(len(train_texts)).batch(8)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the model\n",
        "model.fit(train_dataset, epochs=10)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Colab_Notebooks/Adanomad\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab_Notebooks/Adanomad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oijkTbLDOAk3",
        "outputId": "0ff46409-8a7d-41e4-a31a-82ea90b98fe3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 84s 4s/step - loss: 1.1436 - accuracy: 0.2833\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 38s 5s/step - loss: 1.0131 - accuracy: 0.6000\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 38s 5s/step - loss: 0.7064 - accuracy: 0.9167\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 40s 5s/step - loss: 0.3236 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 53s 7s/step - loss: 0.1080 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 30s 4s/step - loss: 0.0468 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 36s 5s/step - loss: 0.0457 - accuracy: 0.9833\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 32s 4s/step - loss: 0.0768 - accuracy: 0.9833\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 30s 4s/step - loss: 0.0528 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 41s 5s/step - loss: 0.0614 - accuracy: 0.9833\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab_Notebooks/Adanomad/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab_Notebooks/Adanomad/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab_Notebooks/Adanomad/vocab.txt',\n",
              " '/content/drive/MyDrive/Colab_Notebooks/Adanomad/added_tokens.json',\n",
              " '/content/drive/MyDrive/Colab_Notebooks/Adanomad/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evualuation"
      ],
      "metadata": {
        "id": "PKJW16BW790T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data was also generated using chatGPT\n",
        "test_data = [\n",
        "    \"Developed robust and scalable web applications using modern technologies like React.js and Node.js.\",\n",
        "    \"Designed and implemented RESTful APIs to facilitate communication between frontend and backend systems.\",\n",
        "    \"Automated deployment processes using tools like Jenkins, Docker, and Kubernetes to streamline the release cycle.\",\n",
        "    \"Conducted exploratory data analysis to identify patterns and trends in customer behavior.\",\n",
        "    \"Clean, preprocess, and prepare raw data for analysis and modeling.\",\n",
        "    \"Managed project scope, schedule, and resources to ensure successful project outcomes.\",\n",
        "    \"Developed project plans and schedules using tools like Microsoft Project and Smartsheet.\",\n",
        "]\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(test_data)):\n",
        "  test_text = remove_stopwords(test_data[i])\n",
        "\n",
        "  # Tokenize the input text\n",
        "  inputs = tokenizer(test_text, return_tensors=\"tf\")\n",
        "\n",
        "  # Perform inference\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "  # Access logits\n",
        "  logits = outputs.logits\n",
        "\n",
        "  # Convert logits to probabilities\n",
        "  probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  # Access predicted class\n",
        "  predicted_class = tf.argmax(probabilities, axis=-1).numpy()[0]\n",
        "\n",
        "  predictions.append(predicted_class)\n",
        "\n",
        "# True values\n",
        "test_labels = [0, 0, 0, 1, 1, 2, 2]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeMXfHfR5LmY",
        "outputId": "cfbe61d1-ae6f-43cc-f975-f72d081e1e16"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7142857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "mE_J9B2Q1Yuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the saved fine-tuned model directory on your local filesystem\n",
        "fine_tuned_model_path = \"/content/drive/MyDrive/Colab_Notebooks/Adanomad\"\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(fine_tuned_model_path)"
      ],
      "metadata": {
        "id": "mMT0QB0bQMj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d20da4-8245-4da2-a21b-8e49666d4e37"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at /content/drive/MyDrive/Colab_Notebooks/Adanomad were not used when initializing TFBertForSequenceClassification: ['dropout_189']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Colab_Notebooks/Adanomad.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text for classification\n",
        "text = \"Collaborate with cross-functional teams to define project objectives, gather requirements, and deliver data-driven solutions.Develop and deploy machine learning models for forecasting, classification, and recommendation systems. Design and implement data pipelines for data ingestion, preprocessing, and feature engineering..\"\n",
        "text = remove_stopwords(text)\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "\n",
        "# Perform inference\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Access logits\n",
        "logits = outputs.logits\n",
        "\n",
        "# Convert logits to probabilities\n",
        "probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "# Access predicted class\n",
        "predicted_class = tf.argmax(probabilities, axis=-1).numpy()[0]\n",
        "\n",
        "print(\"Predicted class:\", predicted_class)\n",
        "\n",
        "if(predicted_class == 0):\n",
        "  print(\"Job: Software Engineer\")\n",
        "elif(predicted_class == 1):\n",
        "  print(\"Job: Data Scientist\")\n",
        "elif(predicted_class == 2):\n",
        "  print(\"Job: Project Manager\")\n"
      ],
      "metadata": {
        "id": "cz_f7zeUUdfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b7dacc-c8da-4001-b8e7-12c52994f985"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n",
            "Job: Software Engineer\n"
          ]
        }
      ]
    }
  ]
}